{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77697a9a",
   "metadata": {},
   "source": [
    "#### 第五章 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba95e8",
   "metadata": {},
   "source": [
    "##### 分类树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fefbe867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "class Tree:\n",
    "    \"\"\"\n",
    "    决策树\n",
    "    :param nodes 节点\n",
    "    :param axis 维度\n",
    "    :param value 叶节点取值 \n",
    "    \"\"\"\n",
    "    def __init__(self, nodes:dict, axis: int, value:float|None):\n",
    "        self.nodes = nodes\n",
    "        self.axis = axis\n",
    "        self.value = value\n",
    "\n",
    "def entropy(arr:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    计算信息熵\n",
    "    :param Y: 标签，一维列表\n",
    "    :return entropy: 信息熵\n",
    "    \"\"\"\n",
    "    # 统计唯一值及其出现次数\n",
    "    unique_vals, counts = np.unique(arr, return_counts=True)\n",
    "\n",
    "    # 计算概率\n",
    "    total = len(arr)\n",
    "    probability = counts / total\n",
    "\n",
    "    # 计算信息熵\n",
    "    entropy = -np.sum(probability * np.log2(probability))\n",
    "    return entropy\n",
    "\n",
    "def gini(arr:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    计算基尼指数\n",
    "    :param Y: 标签，一维列表\n",
    "    :return gini: 基尼指数\n",
    "    \"\"\"\n",
    "    # 统计唯一值及其出现次数\n",
    "    unique_vals, counts = np.unique(arr, return_counts=True)\n",
    "\n",
    "    # 计算概率\n",
    "    total = len(arr)\n",
    "    probability = counts / total\n",
    "\n",
    "    # 计算信息熵\n",
    "    gini = 1 - np.sum(np.power(probability, 2))\n",
    "    return gini\n",
    "        \n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    决策树算法\n",
    "    :param X: 特征矩阵 (n_samples, n_features)\n",
    "    :param Y: 标签数组 (n_samples,)\n",
    "    :param feat_name: 特征名称\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, feat_name: list[str]):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.feat_name = feat_name\n",
    "\n",
    "    def calculate_info_gain(self, feat:np.ndarray, Y:np.ndarray, metric: Callable[[np.ndarray], float]) -> float:\n",
    "        \"\"\"\n",
    "        计算特征 feat 对标签 Y 的信息增益\n",
    "        信息增益公式：IG = 熵(整体Y) - 条件熵(feat划分后的Y)\n",
    "        \n",
    "        :param feat: 特征数组，一维 np.ndarray（每个样本的特征值）\n",
    "        :param Y: 标签数组，一维 np.ndarray（每个样本的标签）\n",
    "        :param metric: 计算熵/基尼指数的函数，输入一维数组，返回浮点数（\n",
    "        :return float: 信息增益值\n",
    "        \"\"\"\n",
    "        # 计算原始值\n",
    "        base_metric = metric(Y)\n",
    "\n",
    "        # 统计唯一值及其出现次数\n",
    "        unique_feat_vals, feat_counts = np.unique(feat, return_counts=True)\n",
    "        conditional_metric = 0.0\n",
    "        \n",
    "        for val, count in zip(unique_feat_vals, feat_counts):\n",
    "            # 取出该特征值对应的所有标签\n",
    "            y_subset = Y[feat == val]\n",
    "            # 累加\n",
    "            conditional_metric += (count / len(feat)) * metric(y_subset)\n",
    "        \n",
    "        # 4. 计算信息增益\n",
    "        info_gain = base_metric - conditional_metric\n",
    "        return info_gain\n",
    "\n",
    "    def _choose_best_feature(self, X: np.ndarray, Y: np.ndarray, metric: Callable[[np.ndarray], float], ratio: bool=False) -> int:\n",
    "        \"\"\"\n",
    "        选择最优划分特征（信息增益最大的特征）\n",
    "        :param X: 特征矩阵 (n_samples, n_features)\n",
    "        :param Y: 标签数组 (n_samples,)\n",
    "        :param metric: 不纯度计算函数\n",
    "        :param ratio: 是否计算信息增益比\n",
    "        :return: 最优特征的维度索引\n",
    "        \"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        best_gain = -1  # 初始化最优增益\n",
    "        best_feature_idx = -1  # 初始化最优特征索引\n",
    "        \n",
    "        # 遍历所有特征\n",
    "        for idx in range(n_features):\n",
    "            # 取出第idx列特征\n",
    "            feat = X[:, idx]\n",
    "            # 计算该特征的信息增益\n",
    "            gain = self.calculate_info_gain(feat, Y, metric)\n",
    "            if ratio:\n",
    "                gain = gain / metric(feat)\n",
    "            # 更新最优特征\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature_idx = idx\n",
    "        \n",
    "        return best_feature_idx\n",
    "\n",
    "    def _majority_vote(self, Y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        返回出现次数最多的标签（叶节点取值）\n",
    "        :param Y: 标签数组\n",
    "        :return: 最频繁的标签值\n",
    "        \"\"\"\n",
    "        unique_vals, counts = np.unique(Y, return_counts=True)\n",
    "        return unique_vals[np.argmax(counts)]\n",
    "\n",
    "    def fit_tree(self, X:np.ndarray, Y:np.ndarray, metric: Callable[[np.ndarray], float], ratio: bool=False) -> Tree:\n",
    "        \"\"\"\n",
    "        递归构建决策树\n",
    "        :param X: 特征矩阵 (n_samples, n_features)\n",
    "        :param Y: 标签数组 (n_samples,)\n",
    "        :param metric: 不纯度计算函数（entropy/gini）\n",
    "        :param ratio: 是否计算信息增益\n",
    "        :return: 决策树根节点\n",
    "        \"\"\"\n",
    "        # 终止条件1：所有样本标签相同，返回叶节点\n",
    "        if len(np.unique(Y)) == 1:\n",
    "            return Tree(nodes={}, axis=-1, value=Y[0])\n",
    "\n",
    "        # 终止条件2：没有特征可划分（只剩1列特征或所有样本特征相同）\n",
    "        if X.shape[1] == 0 or (X == X[0]).all():\n",
    "            majority_val = self._majority_vote(Y)\n",
    "            return Tree(nodes={}, axis=-1, value=majority_val)\n",
    "        \n",
    "        # 其他\n",
    "        best_feature_idx = self._choose_best_feature(X, Y, metric, ratio)\n",
    "        best_feature = X[:, best_feature_idx]\n",
    "        unique_vals = np.unique(best_feature)\n",
    "        current_tree = Tree(nodes={}, axis=best_feature_idx, value=None)\n",
    "        # 递归构建子树\n",
    "        for val in unique_vals:\n",
    "            mask = (best_feature == val)\n",
    "            X_subset = X[mask]\n",
    "            Y_subset = Y[mask]\n",
    "            # 移除已划分的特征（避免重复使用）\n",
    "            X_subset = np.delete(X_subset, best_feature_idx, axis=1)\n",
    "            # 递归构建子树\n",
    "            current_tree.nodes[val] = self.fit_tree(X_subset, Y_subset, metric)\n",
    "        \n",
    "        return current_tree\n",
    "\n",
    "    def fit(self, metric: Callable[[np.ndarray], float] = None, ratio: bool=False):\n",
    "        \"\"\"\n",
    "        训练决策树\n",
    "        :param metric: 信息计算函数，默认用entropy\n",
    "        :param ratio: 是否计算信息增益\n",
    "        \"\"\"\n",
    "        if metric is None:\n",
    "            metric = entropy\n",
    "        self.tree = self.fit_tree(self.X, self.Y, metric, ratio)\n",
    "    \n",
    "    def predict(self, X: np.ndarray, tree: Tree) -> float:\n",
    "        \"\"\"\n",
    "        递归预测\n",
    "        :param x: 单个样本特征 (n_features,)\n",
    "        :param tree: 决策树根节点\n",
    "        :return: 预测标签\n",
    "        \"\"\"\n",
    "        # 叶节点，直接返回值\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        # 分支节点，找对应的子树\n",
    "        feat_val = X[tree.axis]\n",
    "        x = np.delete(X, tree.axis, axis=0)\n",
    "        return self.predict(x, tree.nodes[feat_val])\n",
    "\n",
    "    def print_tree(self, tree: Tree = None, depth: int = 0, prefix: str = \"根节点: \"):\n",
    "        \"\"\"\n",
    "        递归打印决策树结构\n",
    "        :param tree: 要打印的树节点（默认用训练好的self.tree）\n",
    "        :param depth: 递归深度（用于缩进）\n",
    "        :param prefix: 节点前缀（区分分支/叶节点）\n",
    "        \"\"\"\n",
    "        # 初始化：如果没传tree，用训练好的根节点\n",
    "        if tree is None:\n",
    "            if self.tree is None:\n",
    "                print(\"决策树尚未训练！\")\n",
    "                return\n",
    "            tree = self.tree\n",
    "        \n",
    "        # 缩进符（按深度增加，让结构更清晰）\n",
    "        indent = \"  \" * depth\n",
    "        \n",
    "        # 情况1：叶节点 → 直接打印预测值\n",
    "        if tree.value is not None:\n",
    "            print(f\"{indent}{prefix} 预测结果 = {tree.value}\")\n",
    "            return\n",
    "        \n",
    "        # 情况2：分支节点 → 打印划分特征 + 递归打印子节点\n",
    "        feature_name = self.feat_name[tree.axis]\n",
    "        print(f\"{indent}{prefix} 按【{feature_name}】划分\")\n",
    "        \n",
    "        # 遍历所有子节点\n",
    "        for feat_val, subtree in tree.nodes.items():\n",
    "            self.print_tree(\n",
    "                tree=subtree,\n",
    "                depth=depth + 1,\n",
    "                prefix=f\"当{feature_name} = {feat_val} → \"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d0f3405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根节点:  按【有自己的房子】划分\n",
      "  当有自己的房子 = 否 →  按【有工作】划分\n",
      "    当有工作 = 否 →  预测结果 = 否\n",
      "    当有工作 = 是 →  预测结果 = 是\n",
      "  当有自己的房子 = 是 →  预测结果 = 是\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 特征矩阵 X（保留原始文本，不编码）\n",
    "X = np.array([\n",
    "    [\"青年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"青年\", \"否\", \"否\", \"好\"],\n",
    "    [\"青年\", \"是\", \"否\", \"好\"],\n",
    "    [\"青年\", \"是\", \"是\", \"一般\"],\n",
    "    [\"青年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"中年\", \"否\", \"否\", \"一般\"],\n",
    "    [\"中年\", \"否\", \"否\", \"好\"],\n",
    "    [\"中年\", \"是\", \"是\", \"好\"],\n",
    "    [\"中年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"中年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"是\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"是\", \"好\"],\n",
    "    [\"老年\", \"是\", \"否\", \"好\"],\n",
    "    [\"老年\", \"是\", \"否\", \"非常好\"],\n",
    "    [\"老年\", \"否\", \"否\", \"一般\"]\n",
    "], dtype=str)\n",
    "\n",
    "# 类别 Y（保留原始文本）\n",
    "Y = np.array([\n",
    "    \"否\", \"否\", \"是\", \"是\", \"否\",\n",
    "    \"否\", \"否\", \"是\", \"是\", \"是\",\n",
    "    \"是\", \"是\", \"是\", \"是\", \"否\"\n",
    "], dtype=str)\n",
    "\n",
    "feat_name = [\"年龄\", \"有工作\", \"有自己的房子\", \"信贷情况\"]\n",
    "\n",
    "dt = DecisionTree(X,Y, feat_name)\n",
    "dt.fit(gini)\n",
    "dt.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c73bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根节点:  按【有自己的房子】划分\n",
      "  当有自己的房子 = 否 →  按【有工作】划分\n",
      "    当有工作 = 否 →  预测结果 = 否\n",
      "    当有工作 = 是 →  预测结果 = 是\n",
      "  当有自己的房子 = 是 →  预测结果 = 是\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTree(X,Y, feat_name)\n",
    "dt.fit(gini, ratio=True)\n",
    "dt.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "321a44b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.str_('否')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict([\"老年\", \"否\", \"否\", \"一般\"], dt.tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab5a27",
   "metadata": {},
   "source": [
    "#### 回归树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a48e4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_best_split(feat: np.ndarray, Y:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    寻找回归树的最佳分裂点\n",
    "    :param feat 一维特征数列\n",
    "    :param Y 一维标签数列\n",
    "    :return s分割点\n",
    "    \"\"\"\n",
    "    unique_vals = np.sort(np.unique(feat))\n",
    "    \n",
    "    var = 1e9\n",
    "    for val in unique_vals[:-1]:\n",
    "        idx = (feat <= val)\n",
    "        #计算该分割点下的预测误差\n",
    "        cur_var = np.var(Y[idx]) + np.var(Y[~idx])\n",
    "        \n",
    "        if cur_var < var:\n",
    "            s = val\n",
    "    return s\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64)\n",
    "y = np.array([4.50, 4.75, 4.91, 5.34, 5.80, 7.05, 7.90, 8.23, 8.70, 9.00], dtype=np.float64)\n",
    "find_best_split(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
